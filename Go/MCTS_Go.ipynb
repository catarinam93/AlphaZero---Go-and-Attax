{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d796c9a-ee1e-462e-b8fc-4a179ac49111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0) # set the seed as 0\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956efe2",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search (MCTS) Implementation Documentation\n",
    "\n",
    "### Overview\n",
    "\n",
    "The MCTS algorithm is a decision-making algorithm that uses Monte Carlo simulations to build a tree of possible moves in a game, searching for the most promising actions to take. This implementation consists of two classes: `Node` representing a node in the MCTS tree, and `MCTS` implementing the MCTS algorithm.\n",
    "\n",
    "### Node Class\n",
    "\n",
    "### Key Decisions\n",
    "\n",
    "#### 1. Node Expansion\n",
    "\n",
    "Nodes are expanded based on the policy predicted by the neural network model. Each child represents a possible action. This decision allows the MCTS algorithm to explore potential moves and build a comprehensive tree of possibilities.\n",
    "\n",
    "#### 2. Selection Strategy\n",
    "\n",
    "Nodes are selected based on the Upper Confidence Bound (UCB) score, striking a balance between exploration and exploitation. This strategy ensures that promising nodes are prioritized for further exploration while still considering less-explored options.\n",
    "\n",
    "#### 3. Simulation\n",
    "\n",
    "Simulations involve random rollouts from a node to estimate the value of a state. This decision introduces an element of randomness, allowing the algorithm to assess the potential outcomes of different actions.\n",
    "\n",
    "#### 4. Backpropagation\n",
    "\n",
    "The value is backpropagated up the tree to update visit counts and value estimates. Backpropagation ensures that the information gathered during simulations influences the overall understanding of the state values throughout the tree.\n",
    "\n",
    "#### 5. Terminal States Handling\n",
    "\n",
    "Terminal states are identified during the simulation phase, and values are flipped to account for the perspective of the opponent. This handling of terminal states ensures accurate value assessments, considering the game outcome from the player's perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf35e37",
   "metadata": {},
   "source": [
    "### MCTS with the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ba0e22-4338-4994-b413-c8c9a0061312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, player, parent=None, action_taken=None, prior=0): # =None beacuse of the root node\n",
    "        self.game = game # The game object\n",
    "        self.args = args  # The arguments of the game\n",
    "        self.state = state # The state of the game at this node\n",
    "        self.parent = parent # The parent node of this node\n",
    "        self.action_taken = action_taken # The action that led to this node\n",
    "        self.player = player # The player who made the action\n",
    "        self.prior = prior # The probability of the action taken\n",
    "\n",
    "        self.children = [] # The children of this node\n",
    "\n",
    "        self.visit_count = 0 # The number of times this node has been visited\n",
    "        self.value_sum = 0 # The sum of the values of the children of this node\n",
    "        \n",
    "        # print(f\"Initialized Node: Action={action_taken}, Player={player}, Parent={parent}\")\n",
    "\n",
    "    # Check if the node is fully expanded\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    # Selection phase\n",
    "    def select(self):\n",
    "        # Select the child with the highest ucb score and return it \n",
    "        best_child = None \n",
    "        best_ucb = -np.inf # -inf because we want to maximize the ucb score\n",
    "\n",
    "        # Iterate over the children of this node\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child) # Get the ucb score of the child\n",
    "            if ucb > best_ucb: # If the ucb score is better than the best ucb score\n",
    "                best_child = child  # Set the best child to this child\n",
    "                best_ucb = ucb # Set the best ucb score to this ucb score\n",
    "                \n",
    "        # print(f\"Selected Node: Action={best_child.action_taken}, UCB={best_ucb}\")\n",
    "        return best_child\n",
    "\n",
    "    # Get the ucb score of a child\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0: # If the child has not been visited\n",
    "            q_value = 0 # Set the q value to 0\n",
    "            # return float('inf')\n",
    "        else:\n",
    "            # 1- beacuse the next player to make a move is our opponent so we want to put him on a bad situation therefor the value is close to 0\n",
    "            # +1) / 2 is to become a probability, which means the range is [0,1], before it was [-1,1]\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior #the formula of ucb\n",
    "        # exploitation = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        # exploration = self.args['C'] * math.sqrt(math.log(self.visit_count) / (child.visit_count + 1))\n",
    "        # return exploitation + exploration * child.prior\n",
    "        \n",
    "    # Expansion phase\n",
    "    def expand(self, policy):\n",
    "        # Iterate over the policy\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                # Create a child node with the action and prior probability\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1) # 1 because the player is 1 \n",
    "                child_state = self.game.change_perspective(child_state, -1) # Change the perspective to the opponent\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self.player, self, action, prob) # Create the child node\n",
    "                self.children.append(child) # Add the child node to the children of this node\n",
    "\n",
    "    # Simulation phase\n",
    "    def simulate(self):\n",
    "        # Get the value of the state and if it is terminal or not \n",
    "        value, winner, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken, self.player)\n",
    "        value = -value # -value because we want to maximize the value\n",
    "\n",
    "        if is_terminal:\n",
    "            return value # If the state is terminal return the value\n",
    "\n",
    "        rollout_state = self.state.copy() # Copy the state\n",
    "        rollout_player = 1 # Set the player to 1 \n",
    "        while True:\n",
    "            valid_moves = self.game.get_valid_moves(rollout_state, self.player) # Get the valid moves\n",
    "            action = np.random.choice(valid_moves) # Choose a random valid move\n",
    "            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player) # Get the next state\n",
    "            value, winner, is_terminal = self.game.get_value_and_terminated(rollout_state, action, self.player) # Get the value of the state and if it is terminal or not\n",
    "            \n",
    "            if is_terminal:\n",
    "                if rollout_player == -1:\n",
    "                    value = -value # -value because we want to maximize the value\n",
    "                return value \n",
    "            rollout_player = -rollout_player # Change the player\n",
    "\n",
    "    # Backpropagation phase\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value # Add the value to the value sum\n",
    "        self.visit_count += 1 # Increment the visit count\n",
    "        \n",
    "        value = -value # -value because we want to maximize the value\n",
    "\n",
    "        if self.parent is not None: # If the node has a parent\n",
    "            self.parent.backpropagate(value) # Backpropagate to the parent\n",
    "        # print(f\"Backpropagating Node: Action={self.action_taken}, New Visit Count={self.visit_count}, Value Sum={self.value_sum}\")\n",
    "\n",
    "\n",
    "# The MCTS class \n",
    "class MCTS:\n",
    "    def __init__(self, game, args, player, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.player = player\n",
    "        self.model = model # The model that predicts the policy and value\n",
    "        print(\"Initialized MCTS\")\n",
    "\n",
    "\n",
    "    @torch.no_grad() \n",
    "\n",
    "    # Search phase\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, self.player) # Create the root node\n",
    "        \n",
    "        for search in range(self.args['num_searches']): # Iterate over the number of searches\n",
    "            node = root # Set the node to the root node\n",
    "\n",
    "            # Selection phase\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select() # Select the best child node\n",
    "                \n",
    "            # Check if the node is terminal and backpropagate immediately if it is\n",
    "            value, winner, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken, self.player)\n",
    "            value = -value # -value because we want to maximize the value\n",
    "    \n",
    "            # Check if the node is terminate and backpropagate immediately if not we expand and simulate\n",
    "            if not is_terminal: \n",
    "                # Expansion phase\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0) # Unsqueeze to add a batch dimension\n",
    "                )\n",
    "                # Expand the node with the policy and simulate\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy() # Softmax the policy and convert it to a numpy array\n",
    "                valid_moves = self.game.get_mask(node.state, node.player) # Get the valid moves\n",
    "               \n",
    "                policy *= valid_moves # Mask the policy\n",
    "                policy /= np.sum(policy) # Normalize the policy\n",
    "               \n",
    "                value = value.item() # Get the value as a python number \n",
    "                node.expand(policy) # Expand the node\n",
    "                \n",
    "            # Backpropagation phase\n",
    "            node.backpropagate(value) # Backpropagate the value\n",
    "        \n",
    "        # Get the action probabilities of the root node\n",
    "        action_probs = np.zeros(self.game.action_size + 1) # +1 because of the pass action\n",
    "        # print(\"Final Root Children States and Visit Counts:\")\n",
    "        for child in root.children: # Iterate over the children of the root node\n",
    "            # print(f\"Root Child: Action={child.action_taken}, Visit Count={child.visit_count}\")\n",
    "            action_probs[child.action_taken] = child.visit_count # Set the action probability to the visit count of the child\n",
    "        action_probs /= np.sum(action_probs) # Normalize the action probabilities\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64339f",
   "metadata": {},
   "source": [
    "### MCTS without the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f722abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT THE NEURAL NETWORK\n",
    "\n",
    "# class Node:\n",
    "#     def __init__(self, game, args, state, player, parent=None, action_taken=None): # =None beacuse of the root node\n",
    "#         self.game = game\n",
    "#         self.args = args\n",
    "#         self.state = state\n",
    "#         self.parent = parent\n",
    "#         self.action_taken = action_taken\n",
    "#         self.player = player\n",
    "\n",
    "#         self.children = []\n",
    "#         self.expandable_moves = game.get_valid_moves(state, self.player)  #list\n",
    "\n",
    "#         self.visit_count = 0\n",
    "#         self.value_sum = 0\n",
    "\n",
    "#     #for the expansion\n",
    "#     def is_fully_expanded(self):\n",
    "#         return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "\n",
    "#     #for the selection\n",
    "#     def select(self):\n",
    "#         #look of all of your children and for each child we calculate the ucb score and choose the one with the best score\n",
    "#         best_child = None\n",
    "#         best_ucb = -np.inf\n",
    "\n",
    "#         for child in self.children:\n",
    "#             ucb = self.get_ucb(child)\n",
    "#             if ucb > best_ucb:\n",
    "#                 best_child = child\n",
    "#                 best_ucb = ucb\n",
    "                \n",
    "#         return best_child\n",
    "\n",
    "#     # calculate the ucb score of a node\n",
    "#     def get_ucb(self, child):\n",
    "#         # 1- beacuse the next player to make a move is our opponent so we want to put him on a bad situation therefor the value is close to 0\n",
    "#         # +1) / 2 is to become a probability, which means the range is [0,1], before it was [-1,1]\n",
    "#         q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "#         return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count) / child.visit_count) #the formula of ucb\n",
    "\n",
    "#     #expansion\n",
    "#     def expand(self):\n",
    "#         #== 1 because 1 means the move is legal\n",
    "#         #choose randommly an indice of a move to expand\n",
    "#         action = np.random.choice(self.expandable_moves)\n",
    "#         # print(self.expandable_moves)\n",
    "#         #make this move not expandable anymore\n",
    "#         self.expandable_moves.remove(action)\n",
    "#         # print(action,\":\" ,self.expandable_moves)\n",
    "#         child_state = self.state.copy()\n",
    "#         child_state = self.game.get_next_state(child_state, action, 1) #we never change the player, we flip the state arround \n",
    "#         child_state = self.game.change_perspective(child_state, -1)\n",
    "\n",
    "#         child = Node(self.game, self.args, child_state, self.player, None, action)\n",
    "#         #append the node\n",
    "#         self.children.append(child)\n",
    "#         return child\n",
    "\n",
    "#     #simulation\n",
    "#     def simulate(self):\n",
    "#         #verify if it is terminal\n",
    "#         value, winner, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken, self.player)\n",
    "#         #flip arround\n",
    "#         value = -value\n",
    "\n",
    "#         if is_terminal:\n",
    "#             return value\n",
    "\n",
    "#         rollout_state = self.state.copy()\n",
    "#         rollout_player = 1\n",
    "#         while True:\n",
    "#             valid_moves = self.game.get_valid_moves(rollout_state, self.player)\n",
    "#             # if action is not None and action in valid_moves:\n",
    "#             #     valid_moves.remove(action)\n",
    "#             # print(valid_moves)    \n",
    "#             action = np.random.choice(valid_moves)\n",
    "#             rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "#             # print(rollout_state, action)\n",
    "#             value, winner, is_terminal = self.game.get_value_and_terminated(rollout_state, action, self.player)\n",
    "            \n",
    "#             if is_terminal:\n",
    "#                 if rollout_player == -1:\n",
    "#                     value = -value\n",
    "#                 return value\n",
    "#             #flip the player\n",
    "#             rollout_player = -rollout_player\n",
    "\n",
    "#     #backpropagation\n",
    "#     def backpropagate(self, value):\n",
    "#         self.value_sum += value\n",
    "#         self.visit_count += 1\n",
    "\n",
    "#         value = -value\n",
    "#         if self.parent is not None:\n",
    "#             print(self.parent)\n",
    "#             self.parent.backpropagate(value)\n",
    "\n",
    "\n",
    "      \n",
    "# class MCTS:\n",
    "#     def __init__(self, game, args, player):\n",
    "#         self.game = game\n",
    "#         self.args = args\n",
    "#         self.player = player\n",
    "\n",
    "#     def search(self, state):\n",
    "#         #define the root node\n",
    "#         root = Node(self.game, self.args, state, self.player)\n",
    "        \n",
    "#         #iterations\n",
    "#         for search in range(self.args['num_searches']):\n",
    "#             node = root\n",
    "#             #selection phase\n",
    "#             while node.is_fully_expanded():\n",
    "#                 node = node.select()\n",
    "                \n",
    "#             #check if the node selected is a terminal one \n",
    "#             value, winner, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken, self.player)\n",
    "#             value = -value \n",
    "    \n",
    "#             # check if the node is terminate and backpropagate immediately if not we expand and simulate\n",
    "#             if not is_terminal: \n",
    "#                 #expansion phase\n",
    "#                 node = node.expand()\n",
    "#                 #simulations phase\n",
    "#                 value = node.simulate()\n",
    "                \n",
    "#             #backpropagation phase\n",
    "#             node.backpropagate(value)\n",
    "        \n",
    "#         # return the distibution of visit_counts\n",
    "#         action_probs = np.zeros(self.game.action_size + 1)\n",
    "#         for child in root.children:\n",
    "#             action_probs[child.action_taken] = child.visit_count\n",
    "#         action_probs /= np.sum(action_probs)\n",
    "#         return action_probs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
