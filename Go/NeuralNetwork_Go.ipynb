{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0) # set the seed as 0\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Class\n",
    "\n",
    "### Description\n",
    "\n",
    "The `ResNet` class is a powerful neural network architecture designed for game playing. Inspired by the residual network (ResNet) design principles, this class leverages the idea of residual learning to enable effective training of deep networks. Residual blocks within the architecture facilitate the flow of information across layers, mitigating the vanishing gradient problem and fostering the learning of complex features.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Initial Convolutional Block\n",
    "The model starts with a convolutional block that processes the input game state. This block extracts essential features from the raw pixel values using convolutional layers, batch normalization for stable training, and rectified linear unit (ReLU) activation functions.\n",
    "\n",
    "#### Residual Blocks\n",
    "The backbone of the network consists of a stack of residual blocks. Each block contains two convolutional layers with batch normalization and ReLU activation, ensuring that the model can learn intricate patterns and features from the game environment. The use of residual connections allows for the smooth propagation of gradients during training, facilitating the training of deeper networks.\n",
    "\n",
    "#### Policy Head\n",
    "The policy head is responsible for predicting the probabilities of different actions that the agent can take. It further refines the features extracted by the preceding layers, employing additional convolutional layers, batch normalization, ReLU activation, and a fully connected layer for the final output.\n",
    "\n",
    "#### Value Head\n",
    "The value head focuses on predicting the state value, providing an estimate of the potential reward associated with the current game state. Similar to the policy head, it utilizes convolutional layers, batch normalization, ReLU activation, and a fully connected layer with a hyperbolic tangent (tanh) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "         # Initial convolutional block\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size = 3, padding = 1), # 3 because \"of the three planes ate the beggining\" -1, 0, 1, acho que se matem igual\n",
    "            nn.BatchNorm2d(num_hidden), # Batch normalization for stabilizing training\n",
    "            nn.ReLU() # Rectified Linear Unit activation function\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)] \n",
    "        ) \n",
    "        \n",
    "        # Policy head for predicting actions \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size = 3, padding = 1), \n",
    "            nn.BatchNorm2d(32), # Batch normalization for stabilizing training\n",
    "            nn.ReLU(), # Rectified Linear Unit activation function\n",
    "            nn.Flatten(), # Flatten the output of the previous layer\n",
    "            nn.Linear(32 * game.GRID_SIZE * game.GRID_SIZE, game.action_size + 1) \n",
    "        )\n",
    "\n",
    "        # Value head for predicting the state value\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(3), # Batch normalization for stabilizing training\n",
    "            nn.ReLU(), # Rectified Linear Unit activation function\n",
    "            nn.Flatten(),  # Flatten the output of the previous layer\n",
    "            nn.Linear(3 * game.GRID_SIZE * game.GRID_SIZE, 1), # Linear layer with 1 output neuron \n",
    "            nn.Tanh() # Hyperbolic tangent activation function\n",
    "        )\n",
    "\n",
    "        # Send the model to the device (CPU or GPU)\n",
    "        self.to(device)\n",
    "        \n",
    "    # The `forward` method of the `ResNet` class is responsible \n",
    "    # for performing a forward pass through the neural network, transforming the input game state \n",
    "    # into policy and value predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x) # Initial convolutional block\n",
    "        for resBlock in self.backBone: \n",
    "            x = resBlock(x) \n",
    "        policy_logits = self.policyHead(x) # Raw policy logits\n",
    "        value = self.valueHead(x) # Value head for predicting the state value\n",
    "\n",
    "        # Normalize policy logits to probabilities\n",
    "        policy = F.softmax(policy_logits, dim=1)\n",
    "\n",
    "        # Check for NaN values in policy and value outputs\n",
    "        if torch.isnan(policy).any() or torch.isnan(value).any():\n",
    "            raise ValueError(\"NaN value detected in network output\")\n",
    "\n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResBlock Class\n",
    "\n",
    "### Description\n",
    "\n",
    "The `ResBlock` class represents a residual block, a fundamental building block in deep neural network architectures, specifically employed in the `ResNet` model for game playing. Residual blocks enable the training of very deep networks by mitigating the vanishing gradient problem and facilitating the flow of information through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block class \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        # First convolutional layer of the residual block\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding = 1) \n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden) # Batch normalization for stabilizing training\n",
    "\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden) # Batch normalization for stabilizing training\n",
    "\n",
    "    # Residual connection\n",
    "    def forward(self, x):\n",
    "        residual = x # Save the input value\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # First convolutional layer of the residual block with ReLU activation function and batch normalization\n",
    "        x = self.bn2(self.conv2(x)) # Second convolutional layer of the residual block with batch normalization\n",
    "        x += residual # Add the input value to the output of the second convolutional layer\n",
    "        x = F.relu(x) # ReLU activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoRNN Class Documentation\n",
    "\n",
    "### Description\n",
    "\n",
    "The `GoRNN` class is a recurrent neural network (LSTM) designed for processing sequential input data, specifically tailored for the game of Go. It utilizes an LSTM layer to capture temporal dependencies in the input sequence and includes fully connected layers for policy and value predictions. This architecture makes the `GoRNN` model well-suited for learning and predicting patterns in the evolving states of the Go game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model for Go \n",
    "class GoRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, policy_size, value_size):\n",
    "        super(GoRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size # Hidden size of the LSTM\n",
    "        self.num_layers = num_layers # Number of layers of the LSTM\n",
    "\n",
    "        # Define an LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Define fully connected layers for policy and value outputs\n",
    "        self.policy_fc = nn.Linear(hidden_size, policy_size) # Linear layer for policy output\n",
    "        self.value_fc = nn.Linear(hidden_size, value_size) # Linear layer for value output\n",
    "\n",
    "    # The `forward` method of the `GoRNN` class is responsible\n",
    "    # for performing a forward pass through the neural network, transforming the input game state\n",
    "    # into policy and value predictions.\n",
    "    def forward(self, x):\n",
    "        # Check if x needs to be flattened (for 4D input)\n",
    "        if x.dim() == 4:\n",
    "            batch_size, seq_length = x.size(0), x.size(1) # Get batch size and sequence length\n",
    "            x = x.view(batch_size, seq_length, -1)  # Flatten the spatial dimensions\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # Initialize hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # Initialize cell state\n",
    "\n",
    "        # Forward propagate through the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0)) # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step for policy and value\n",
    "        policy = self.policy_fc(out[:, -1, :]) # Get policy output\n",
    "        value = self.value_fc(out[:, -1, :]) # Get value output\n",
    "        return policy, value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
