{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AlphaZero Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0) # set the seed as 0\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run MCTS_Go.ipynb\n",
    "%run NeuralNetwork_Go.ipynb    # CNN model in Attax game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaZero Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, an AlphaZero algorithm is implemented and evaluated for playing Go. The evaluation process focuses on assessing both the algorithm's learning and gameplay performance, as well as the efficiency and effectiveness of its learning process. The primary evaluation metrics used are Loss Metrics, which are monitored during training to gauge how well the model learns from self-play data. Additionally, TensorBoard is utilized for data analysis and visualization.\n",
    "\n",
    "The Loss Metrics include:\n",
    "\n",
    "**Policy Loss:**\n",
    "- This loss measures how well the policy head of your neural network predicts the correct action to take at each step. In the context of games, the policy generally represents the probability distribution over possible moves.\n",
    "- The policy loss is typically calculated using a cross-entropy loss function between the predicted probabilities and the actual distribution of moves from the self-play data.\n",
    "- In AlphaZero, the policy network guides the search by providing a prior probability to the Monte Carlo Tree Search (MCTS) algorithm.\n",
    "\n",
    " **Value Loss:**\n",
    "- The value loss measures how accurately the value head of the neural network estimates the expected outcome (win, loss, or draw) from a given board state.\n",
    "- It is usually calculated using mean squared error (MSE) loss, which compares the predicted value to the actual game outcome.\n",
    "- In AlphaZero, the value estimate helps the MCTS evaluate board states without having to simulate all the way to the end of the game.\n",
    "\n",
    "**Total Loss:**\n",
    "- The total loss is the sum of the policy loss and the value loss. It represents the overall performance of the neural network in both predicting the next best move (policy) and estimating the game's outcome from the current position (value).\n",
    "- Minimizing the total loss is the goal of the training process, as it leads to improvements in the model's policy and value predictions, which should translate into stronger gameplay performance.\n",
    "\n",
    "\n",
    "The code processes data as follows:\n",
    "\n",
    "Model predictions are obtained by passing the current game state through the model (self.model(state)).\n",
    "policy_loss is calculated by comparing the model's policy output to target policy probabilities using cross-entropy loss.\n",
    "value_loss is computed by comparing the model's value output to the actual game outcome using mean squared error loss.\n",
    "The loss variable represents the total loss, which is the sum of policy_loss and value_loss, and it needs to be minimized.\n",
    "The methods zero_grad(), backward(), and step() are used to clear old gradients, compute gradients, and update model parameters during optimization.\n",
    "The code also maintains lists (policy_losses, value_losses, total_losses) to store losses for visualization during the training process.\n",
    "\n",
    "This code is encapsulated within a Python class called AlphaZero, which contains methods for self-play, training, visualization of losses, and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlphaZero:\n",
    "\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.game = game \n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, 1, model)\n",
    "\n",
    "         #to start the tensorboard use this command in the terminal: tensorboard --logdir=runs\n",
    "        self.writer = SummaryWriter('runs/alphazero_experiment')      \n",
    "\n",
    "    # def create_mov(self, action, player):\n",
    "    #     # Extracts xi, yi, xf, yf from action\n",
    "    #     xi = int(action[0])\n",
    "    #     yi = int(action[1])\n",
    "    #     xf = int(action[3])\n",
    "    #     yf = int(action[4])\n",
    "    \n",
    "    #     mov = game.movement(xi, yi, xf, yf, player)\n",
    "    #     return mov\n",
    "    \n",
    "    # Function to play a game of self-play\n",
    "    def selfPlay(self):\n",
    "        self.game = GoGame(9)  # Reset the game state\n",
    "        player = 1\n",
    "        self.mcts = MCTS(self.game, self.args, player, self.model)  # Reset MCTS\n",
    "\n",
    "        memory = []  # List to store the game memory\n",
    "        player = 1  # Player to start the game\n",
    "        state = self.game.get_initial_state()  # Get the initial state of the game\n",
    "        last_move = None  # Variable to store the last move\n",
    "\n",
    "        move_count = 0  # Track the number of moves\n",
    "        top_move_count = 0  # Track how often the top move is chosen\n",
    "\n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            valid_moves = self.game.get_valid_moves(state, player)\n",
    "            action_probs = [prob if index in valid_moves else 0 for index, prob in enumerate(action_probs)]\n",
    "            total_prob = sum(action_probs)\n",
    "            if total_prob > 0:\n",
    "                action_probs = [prob / total_prob for prob in action_probs]\n",
    "\n",
    "            action = np.random.choice(self.game.action_size + 1, p=action_probs)\n",
    "            print(action_probs)\n",
    "\n",
    "            # Check if the current move is the same as the last move\n",
    "            if action == 81 and last_move == 81:  # Assuming 81 is the 'pass' move\n",
    "                break  # Break out of the loop if two consecutive passes occur\n",
    "\n",
    "            move_count += 1\n",
    "            if action == np.argmax(action_probs):\n",
    "                top_move_count += 1\n",
    "\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            value, _, is_terminal = self.game.get_value_and_terminated(state, action, player)\n",
    "\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            if is_terminal:\n",
    "                break  # Break out of the loop if the game reaches a terminal state\n",
    "\n",
    "            last_move = action  # Update the last move\n",
    "            player = -player  # Switch player\n",
    "\n",
    "        game_length = move_count\n",
    "        top_move_ratio = top_move_count / move_count if move_count > 0 else 0\n",
    "\n",
    "        returnMemory = []\n",
    "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "            hist_outcome = value if hist_player == player else -value  # Negate value for opponent\n",
    "            returnMemory.append((\n",
    "                self.game.get_encoded_state(hist_neutral_state),\n",
    "                hist_action_probs,\n",
    "                hist_outcome\n",
    "            ))\n",
    "\n",
    "        return returnMemory, value, game_length, top_move_ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Function to train the model\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory) # Shuffle the training data\n",
    "\n",
    "        # Initialize lists to store losses for visualization\n",
    "        policy_losses = [] \n",
    "        value_losses = []\n",
    "        total_losses = []\n",
    "\n",
    "        # Loop through the memory in batches \n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']): \n",
    "            sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])] # Get the batch of data\n",
    "            state, policy_targets, value_targets = zip(*sample) # Transpose the data\n",
    "\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1) # Convert the data to numpy arrays\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32) # Convert the state to a tensor\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32) # Convert the policy targets to a tensor\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32) # Convert the value targets to a tensor\n",
    "\n",
    "            out_policy, out_value = self.model(state) # Get the output policy and value from the model\n",
    " \n",
    "            # Calculate losses\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets) \n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            total_loss = policy_loss + value_loss\n",
    "\n",
    "\n",
    "            # Log losses\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            total_losses.append(total_loss.item())\n",
    "\n",
    "            # Backpropagate and optimize the model\n",
    "            self.optimizer.zero_grad() \n",
    "            total_loss.backward() \n",
    "            self.optimizer.step()\n",
    "\n",
    "        return policy_losses, value_losses, total_losses # Return the losses\n",
    "\n",
    "    # Function to visualize the losses\n",
    "    def visualize_losses(self, policy_losses, value_losses, total_losses):\n",
    "        plt.figure(figsize=(10, 5)) # Set the figure size\n",
    "        plt.plot(policy_losses, label='Policy Loss') # Plot the policy losses\n",
    "        plt.plot(value_losses, label='Value Loss') # Plot the value losses\n",
    "        plt.plot(total_losses, label='Total Loss')  # Plot the total losses\n",
    "        plt.xlabel('Training Steps') # Set the x label\n",
    "        plt.ylabel('Loss') # Set the y label\n",
    "        plt.title('Training Loss Over Time') # Set the title\n",
    "        plt.legend() # Show the legend\n",
    "        plt.show() # Show the plot\n",
    "\n",
    "    # Function to visualize the performance\n",
    "    def visualize_performance(self, win_rates):\n",
    "        plt.figure(figsize=(10, 5)) # Set the figure size\n",
    "        plt.plot(win_rates, label='Win Rate') # Plot the win rates\n",
    "        plt.xlabel('Iterations') # Set the x label\n",
    "        plt.ylabel('Win Rate') # Set the y label\n",
    "        plt.title('Win Rate Over Iterations') # Set the title\n",
    "        plt.legend() # Show the legend\n",
    "        plt.show() # Show the plot\n",
    "\n",
    "\n",
    "    # Function to learn the model\n",
    "    def learn(self):\n",
    "            \n",
    "            # For all iterations of learning\n",
    "            for iterations in range(self.args['num_iterations']):\n",
    "\n",
    "                memory = [] # List to store the game memory\n",
    "                outcomes = []  # List to store the outcomes of each game\n",
    "                game_lengths = []  # Store the length of each game\n",
    "                top_move_ratios = []  # Store the move quality metric for each game\n",
    "\n",
    "\n",
    "\n",
    "                self.model.eval() # Set the model to evaluation mode\n",
    "                for _ in trange(self.args['num_selfPlay_iterations']): # For each iteration of self-play\n",
    "                    \n",
    "                    game_memory, game_outcome, game_length, top_move_ratio = self.selfPlay() # Play a game of self-play\n",
    "                    memory += game_memory # Append the game memory to the memory list\n",
    "                    outcomes.append(game_outcome) # Append the outcome of the game\n",
    "                    game_lengths.append(game_length) # Append the length of the game\n",
    "                    top_move_ratios.append(top_move_ratio) # Append the move quality metric\n",
    "\n",
    "\n",
    "                self.model.train() # Set the model to training mode\n",
    "\n",
    "                # Calculate and log the averages\n",
    "                avg_game_length = sum(game_lengths) / len(game_lengths)\n",
    "                avg_top_move_ratio = sum(top_move_ratios) / len(top_move_ratios)\n",
    "\n",
    "                \n",
    "                # Calculate the win rate ->\n",
    "                if len(outcomes) > 0: # If there are outcomes\n",
    "                    wins = outcomes.count(1)  # Count the number of wins \n",
    "                    losses = outcomes.count(-1) # Count the number of losses\n",
    "                    draws = outcomes.count(0) # Count the number of draws\n",
    "                    win_rate = wins / len(outcomes)  # Calculate the win rate\n",
    "                else: # If there are no outcomes\n",
    "                    win_rate = 0 # Set the win rate to 0\n",
    "\n",
    "                print(f\"Iteration {iterations}: Win rate: {win_rate*100:.2f}%\") # Print the win rate\n",
    "\n",
    "\n",
    "\n",
    "                # Use TensorBoard to log these metrics\n",
    "                self.writer.add_scalar('Performance/Average_Game_Length', avg_game_length, iterations)\n",
    "                self.writer.add_scalar('Performance/Average_Top_Move_Ratio', avg_top_move_ratio, iterations)\n",
    "                self.writer.add_scalar('Performance/Win_Rate', win_rate, iterations)\n",
    "\n",
    "\n",
    "                # Collect all loss metrics\n",
    "                all_policy_losses = []\n",
    "                all_value_losses = []\n",
    "                all_total_losses = []\n",
    "                all_win_rates = []\n",
    "\n",
    "                # For each epoch of training\n",
    "                for _ in trange(self.args['num_epochs']):\n",
    "                    policy_losses, value_losses, total_losses = self.train(memory) # Train the model\n",
    "                    all_policy_losses.extend(policy_losses) # Append the policy losses\n",
    "                    all_value_losses.extend(value_losses) # Append the value losses\n",
    "                    all_total_losses.extend(total_losses) # Append the total losses\n",
    "                    all_win_rates.append(win_rate) # Append the win rate\n",
    "\n",
    "                # At the end of all epochs, visualize the losses, win rates\n",
    "                self.visualize_losses(all_policy_losses, all_value_losses, all_total_losses)\n",
    "                self.visualize_performance(all_win_rates)\n",
    "\n",
    "                # Save the model after each iteration of learning\n",
    "                torch.save(self.model.state_dict(), f\"model_{iterations}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"optimizer_{iterations}.pt\")\n",
    "\n",
    "\n",
    "            self.writer.close()  # Close the TensorBoard writer\n",
    "\n",
    "                \n",
    "\n",
    "#O QUE ESTAVA\n",
    "    # def learn(self):\n",
    "    #     for iterations in range(self.args['num_iterations']):\n",
    "    #         memory = []\n",
    "\n",
    "    #         self.model.eval()\n",
    "\n",
    "    #         for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']): #trange was used so that we can visualise the progress bars\n",
    "    #             memory += self.selfPlay()\n",
    "\n",
    "    #         self.model.train() \n",
    "    #         for epoch in trange(self.args['num_epochs']):\n",
    "    #             self.train(memory)\n",
    "\n",
    "    #         #store the weights of the model\n",
    "    #         torch.save(self.model.state_dict(), f\"model_{iterations}.pt\")\n",
    "    #         torch.save(self.optimizer.state_dict(), f\"optimizer_{iterations}.pt\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old tensorboard code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use TensorBoard with your AlphaZero implementation, you'll need to make use of the TensorBoard logging through `SummaryWriter` from the `torch.utils.tensorboard` module. Below is a step-by-step guide to integrate TensorBoard into your existing AlphaZero code.\n",
    "\n",
    "# 1. **Install TensorBoard**:\n",
    "#    Ensure you have TensorBoard installed in your environment. If not, you can install it via pip:\n",
    "\n",
    "#     ```bash\n",
    "#     pip install tensorboard\n",
    "#     ```\n",
    "\n",
    "# 2. **Import SummaryWriter**:\n",
    "#    At the beginning of your script, import the necessary TensorBoard class:\n",
    "\n",
    "#     ```python\n",
    "#     from torch.utils.tensorboard import SummaryWriter\n",
    "#     ```\n",
    "\n",
    "# 3. **Initialize SummaryWriter**:\n",
    "#    Create a `SummaryWriter` instance at the beginning of your training script. This object will be used to write logs into a directory that TensorBoard will later read from.\n",
    "\n",
    "#     ```python\n",
    "#     writer = SummaryWriter('runs/alphazero_experiment_1')\n",
    "#     ```\n",
    "\n",
    "# 4. **Log Data**:\n",
    "#    Throughout your training loop and other functions, use the `writer` to log data, such as loss, accuracy, or custom metrics. Here's how you might incorporate it into your AlphaZero training loop:\n",
    "\n",
    "#     ```python\n",
    "#     def train(self, memory):\n",
    "#         # ... [your existing code] ...\n",
    "        \n",
    "#         # Loop over batches\n",
    "#         for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "#             # ... [your existing code] ...\n",
    "            \n",
    "#             # After optimizer step, log the losses\n",
    "#             writer.add_scalar('Loss/policy', policy_loss.item(), global_step)\n",
    "#             writer.add_scalar('Loss/value', value_loss.item(), global_step)\n",
    "#             writer.add_scalar('Loss/total', loss.item(), global_step)\n",
    "            \n",
    "#             # Increment your global step counter\n",
    "#             global_step += 1\n",
    "#     ```\n",
    "\n",
    "# 5. **Log Custom Metrics and Visualizations**:\n",
    "#    Besides scalar values, you might want to log histograms of parameters, images of the game board, or distributions of move probabilities:\n",
    "\n",
    "#     ```python\n",
    "#     # Log parameters (histograms)\n",
    "#     for name, param in self.model.named_parameters():\n",
    "#         writer.add_histogram(name, param, global_step)\n",
    "\n",
    "#     # Log example game states as images\n",
    "#     # Convert your game state to an image (assuming you have a function for this)\n",
    "#     img = game_state_to_image(state)  \n",
    "#     writer.add_image('Game/Board', img, global_step)\n",
    "#     ```\n",
    "\n",
    "# 6. **Start TensorBoard**:\n",
    "#    Once your script is running and logging data, start TensorBoard in a terminal pointing it to the directory where the logs are being written:\n",
    "\n",
    "#     ```bash\n",
    "#     tensorboard --logdir=runs\n",
    "#     ```\n",
    "\n",
    "# 7. **View Your Logs**:\n",
    "#    Open your browser and go to `localhost:6006` (or the URL provided in the terminal when you start TensorBoard) to view the logs and visualizations.\n",
    "\n",
    "# 8. **Close SummaryWriter**:\n",
    "#    At the end of training, or when you're done logging data, ensure to close the SummaryWriter to flush any remaining outputs to disk:\n",
    "\n",
    "#     ```python\n",
    "#     writer.close()\n",
    "#     ```\n",
    "\n",
    "# By following these steps, you'll be able to integrate TensorBoard into your AlphaZero model for rich logging and visualization capabilities. This will help you monitor your training process, understand your model's behavior, and make informed decisions to improve its performance. Remember to customize the logging according to what's most relevant for your specific scenario and model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
