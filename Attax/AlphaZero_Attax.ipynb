{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AlphaZero Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0) # set the seed as 0\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run MCTS_Attax.ipynb\n",
    "%run NeuralNetwork_Attax.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlphaZero Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When evaluating an AlphaZero algorithm it's important to focus on both the performance of the algorithm in learning and playing the game, as well as the efficiency and effectiveness of its learning process. The performance evalutaion metric used was the follow: Loss Metrics: Monitor the policy loss and value loss during training to understand how well the model is learning and generalizing from the self-play data. For analyzing and visualing data it was created a TensorBoard Session.\n",
    "\n",
    "\n",
    "The graphic representation contains:\n",
    "\n",
    " **Policy Loss:**\n",
    "   - This loss measures how well the policy head of your neural network predicts the correct action to take at each step. In the context of games, the policy generally represents the probability distribution over possible moves.\n",
    "   - The policy loss is typically calculated using a cross-entropy loss function between the predicted probabilities and the actual distribution of moves from the self-play data.\n",
    "   - In AlphaZero, the policy network guides the search by providing a prior probability to the Monte Carlo Tree Search (MCTS) algorithm.\n",
    "\n",
    " **Value Loss:**\n",
    "   - The value loss measures how accurately the value head of the neural network estimates the expected outcome (win, loss, or draw) from a given board state.\n",
    "   - It is usually calculated using mean squared error (MSE) loss, which compares the predicted value to the actual game outcome.\n",
    "   - In AlphaZero, the value estimate helps the MCTS evaluate board states without having to simulate all the way to the end of the game.\n",
    "\n",
    "**Total Loss:**\n",
    "   - The total loss is the sum of the policy loss and the value loss. It represents the overall performance of the neural network in both predicting the next best move (policy) and estimating the game's outcome from the current position (value).\n",
    "   - Minimizing the total loss is the goal of the training process, as it leads to improvements in the model's policy and value predictions, which should translate into stronger gameplay performance.\n",
    "\n",
    "\n",
    "- The model's predictions are obtained by passing the current state through the model (`self.model(state)`).\n",
    "- The `policy_loss` is computed by comparing the model's policy output to the target policy probabilities using cross-entropy loss.\n",
    "- The `value_loss` is computed by comparing the model's value output to the actual game outcome using mean squared error loss.\n",
    "- The `loss` variable is the sum of `policy_loss` and `value_loss`, and represents the total loss that needs to be minimized.\n",
    "- The `zero_grad()` method clears old gradients, the `loss.backward()` computes the gradient of the loss with respect to the model parameters, and `optimizer.step()` updates the model parameters to minimize the loss.\n",
    "\n",
    "The losses are appended to lists (`policy_losses`, `value_losses`, `total_losses`) which can be used for visualization, to monitor the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping and validation in machine learning are techniques used to prevent overfitting, which occurs when a model learns the training data too well, including noise and details that do not generalize to unseen data. Here's how they work in our AlphaZero implementation:\n",
    "\n",
    "### Early Stopping:\n",
    "\n",
    "Early stopping monitors the model's performance on a validation set during training. A validation set is a portion of the data that the model has never seen during training and is used solely to assess the model's generalization performance.\n",
    "\n",
    "Here's the flow of early stopping in our code:\n",
    "\n",
    "It started by defining the best validation loss as infinity and setting patience, which is the number of epochs to wait for an improvement in the validation loss before stopping the training.\n",
    "\n",
    "During each epoch of training, after updating the model with the training data (the self-play games in your case), it was calculated the loss on the validation set.\n",
    "\n",
    "The `calculate_validation_loss` function does a forward pass of the validation data through the model and computes the loss without performing any backpropagation or updating the model's weights.\n",
    "\n",
    "It was compared the current validation loss to the best validation loss seen so far. If the current validation loss is lower, it means the model is performing better on the validation set, and then is updated the best validation loss with this new value and reset the patience counter.\n",
    "If the current validation loss doesn't improve, is incremented a counter. If this counter reaches the patience threshold set earlier, is triggered early stopping, meaning to stop training to avoid overfitting.\n",
    "\n",
    "### Validation:\n",
    "\n",
    "Validation is performed by setting aside a part of the dataset that the model does not learn from during the training process. This set is used to evaluate the model's performance and to ensure that it can generalize well to new, unseen data.\n",
    "\n",
    "In our code, it was splited the collected self-play memory into training and validation sets. The model learns from the training set but is evaluated on the validation set. By evaluating the model on data it has not learned from, it can assess how well the model is expected to perform in a real-world scenario or when playing new games.\n",
    "\n",
    "Together, early stopping and validation help ensure that your AlphaZero model doesn't just memorize the training data but learns patterns that are generalizable to new data it hasn't seen before, which is crucial for creating a robust AI player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, 1, model)\n",
    "        self.current_state = self.game.get_initial_state()\n",
    "\n",
    "        #to start the tensorboard use this command in the terminal: tensorboard --logdir=runs\n",
    "        self.writer = SummaryWriter('runs/alphazero_experiment')      \n",
    "\n",
    "\n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        \n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            \n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            if not action_probs: # if it is not empty which means if the game is not over\n",
    "                action = np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
    "                state = self.game.get_next_state(state, action)\n",
    "                value = 0\n",
    "                is_terminal = False\n",
    "\n",
    "            else:\n",
    "                value = 1\n",
    "                is_terminal = True\n",
    "            \n",
    "\n",
    "            if is_terminal:\n",
    "\n",
    "                returnMemory = []\n",
    "\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    # print(\"ciclo for\")\n",
    "                    hist_outcome = value if hist_player == player else -value\n",
    "\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "\n",
    "\n",
    "                return returnMemory\n",
    "            \n",
    "            player = -player\n",
    "\n",
    "    def update_state(self, move_opponent):\n",
    "\n",
    "        # Assumindo que a jogada do oponente é uma string no formato \"x1y1_x2y2\"\n",
    "\n",
    "        # Dividir a string para obter as coordenadas iniciais e finais\n",
    "        coords = move_opponent.split('_')\n",
    "        xi, yi = int(coords[0][0]), int(coords[0][1])\n",
    "        xf, yf = int(coords[1][0]), int(coords[1][1])\n",
    "\n",
    "        # Criar um objeto de movimento\n",
    "        opponent_move = self.game.movement(xi, yi, xf, yf, -1, 0)  # -1 para oponente, tipo 0 inicialmente\n",
    "\n",
    "        # Verificar se o movimento é válido e atualizar o tipo de movimento\n",
    "        if self.game.is_valid_move(self.current_state, opponent_move):\n",
    "            # Aplicar o movimento ao estado atual do jogo\n",
    "            self.current_state = self.game.get_next_state(self.current_state, opponent_move)\n",
    "        else:\n",
    "            # Lidar com movimentos inválidos (por exemplo, ignorar ou registrar um erro)\n",
    "            pass\n",
    "        \n",
    "\n",
    "    def train(self, memory):\n",
    "\n",
    "        global_step = 0\n",
    "        #shuffle our trainning data\n",
    "        random.shuffle(memory)\n",
    "\n",
    "        # Initialize lists to store losses for visualization\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        total_losses = []\n",
    "\n",
    "        #loop over batches\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "\n",
    "            sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample) #transpose our sample arround\n",
    "\n",
    "            statee, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device = self.model.device)\n",
    "    \n",
    "            policy_values = [list(policy_dict.values()) for policy_dict in policy_targets]\n",
    "           \n",
    "            policy_targets = torch.tensor(policy_values, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device = self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # Log losses to TensorBoard\n",
    "            self.writer.add_scalar('Loss/Policy', policy_loss.item(), global_step)\n",
    "            self.writer.add_scalar('Loss/Value', value_loss.item(), global_step)\n",
    "            self.writer.add_scalar('Loss/Total', loss.item(), global_step)\n",
    "\n",
    "\n",
    "            # Log losses\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            total_losses.append(loss.item())\n",
    "            \n",
    "            #minimize the loss by backpropagating it\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "             # Increment the global step count\n",
    "            global_step += 1\n",
    "\n",
    "        return policy_losses, value_losses, total_losses\n",
    "\n",
    "    def visualize_losses(self, policy_losses, value_losses, total_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(policy_losses, label='Policy Loss')\n",
    "        plt.plot(value_losses, label='Value Loss')\n",
    "        plt.plot(total_losses, label='Total Loss')\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "  \n",
    "    \n",
    "    def calculate_validation_loss(self, validation_memory):\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        # Define your loss functions based on your model's output and targets\n",
    "        value_criterion = torch.nn.MSELoss(reduction='sum')\n",
    "        policy_criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients for validation\n",
    "            # Ensure validation_memory is a list of batches\n",
    "            for batch in validation_memory:  # Unpack each batch directly\n",
    "                \n",
    "                states, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "\n",
    "                # Convert to tensors\n",
    "                states_tensor = torch.tensor(states, dtype=torch.float32, device=self.model.device)\n",
    "                policy_values = [list(policy_dict.values()) for policy_dict in policy_targets]\n",
    "                policy_targets_tensor = torch.tensor(policy_values, dtype=torch.float32, device=self.model.device)\n",
    "                value_targets_tensor = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "                # Forward pass\n",
    "                out_policy, out_value = self.model(states_tensor)\n",
    "\n",
    "                # Calculate loss\n",
    "                value_loss = value_criterion(out_value, value_targets_tensor)\n",
    "                policy_loss = policy_criterion(out_policy, policy_targets_tensor)\n",
    "                loss = value_loss + policy_loss\n",
    "\n",
    "                # Accumulate the loss\n",
    "                total_loss += loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "        average_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "        return average_loss\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        all_policy_losses = []\n",
    "        all_value_losses = []\n",
    "        all_total_losses = []\n",
    "\n",
    "         # Early stopping criteria\n",
    "        best_validation_loss = float('inf')\n",
    "        patience = 10  \n",
    "        patience_counter = 0\n",
    "\n",
    "        for iterations in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            self.model.eval()\n",
    "\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']): #trange was used so that we can visualise the progress bars\n",
    "                \n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            # Shuffle and split memory for training and validation\n",
    "            random.shuffle(memory)\n",
    "            split_index = int(len(memory) * 0.9)\n",
    "            train_memory = memory[:split_index]\n",
    "            validation_memory_unbatched = memory[split_index:]\n",
    "            validation_memory = [validation_memory_unbatched[i:i + self.args['batch_size']] for i in range(0, len(validation_memory_unbatched), self.args['batch_size'])]\n",
    "\n",
    "\n",
    "            self.model.train() \n",
    "\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "\n",
    "\n",
    "                # Train on the memory from self-play\n",
    "                policy_losses, value_losses, total_losses = self.train(train_memory)\n",
    "                \n",
    "                all_policy_losses.extend(policy_losses)\n",
    "                all_value_losses.extend(value_losses)\n",
    "                all_total_losses.extend(total_losses)\n",
    "\n",
    "                # Calculate validation loss\n",
    "                current_validation_loss = self.calculate_validation_loss(validation_memory)\n",
    "\n",
    "                self.writer.add_scalar('Loss/Validation', current_validation_loss, iterations)\n",
    "\n",
    "\n",
    "                # Early stopping check\n",
    "                if current_validation_loss < best_validation_loss:\n",
    "                    best_validation_loss = current_validation_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "\n",
    "\n",
    "                # At the end of all epochs, visualize the losses, win rates\n",
    "            self.visualize_losses(all_policy_losses, all_value_losses, all_total_losses)\n",
    "\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"model{iterations}Attax{self.game.N}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer{iterations}Attax{self.game.N}.pt\")\n",
    "\n",
    "\n",
    "        self.writer.close()  # Close the TensorBoard writer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Versões antigas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlphaZero:\n",
    "#     def __init__(self, model, optimizer, game, args):\n",
    "#         self.model = model\n",
    "#         self.game = game\n",
    "#         self.optimizer = optimizer\n",
    "#         self.args = args\n",
    "#         self.mcts = MCTS(game, args, 1, model)\n",
    "\n",
    "#     def selfPlay(self):\n",
    "#         memory = []\n",
    "#         player = 1\n",
    "#         state = self.game.get_initial_state()\n",
    "\n",
    "#         while True:\n",
    "#             neutral_state = self.game.change_perspective(state, player)\n",
    "            \n",
    "#             action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "#             memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "#             if not action_probs: # if it is not empty which means if the game is not over\n",
    "#                 action = np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
    "#                 state = self.game.get_next_state(state, action)\n",
    "#                 value = 0\n",
    "#                 is_terminal = False\n",
    "\n",
    "#             else:\n",
    "#                 value = 1\n",
    "#                 is_terminal = True\n",
    "\n",
    "            \n",
    "#             if is_terminal:\n",
    "#                 returnMemory = []\n",
    "#                 for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "#                     # print(\"ciclo for\")\n",
    "#                     hist_outcome = value if hist_player == player else -value\n",
    "#                     returnMemory.append((\n",
    "#                         self.game.get_encoded_state(neutral_state),\n",
    "#                         hist_action_probs,\n",
    "#                         hist_outcome\n",
    "#                     ))\n",
    "#                 return returnMemory\n",
    "            \n",
    "#             player = -player\n",
    "\n",
    "#     def train(self, memory):\n",
    "#         #shuffle our trainning data\n",
    "#         random.shuffle(memory)\n",
    "\n",
    "#         #loop over batches\n",
    "#         for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "#             sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])]\n",
    "#             state, policy_targets, value_targets = zip(*sample) #transpose our sample arround\n",
    "\n",
    "#             state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "#             state = torch.tensor(state, dtype=torch.float32, device = self.model.device)\n",
    "#             # print(policy_targets)\n",
    "#             # policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device = self.model.device)\n",
    "#             policy_values = [list(policy_dict.values()) for policy_dict in policy_targets]\n",
    "#             policy_targets = torch.tensor(policy_values, dtype=torch.float32, device=self.model.device)\n",
    "#             value_targets = torch.tensor(value_targets, dtype=torch.float32, device = self.model.device)\n",
    "\n",
    "#             out_policy, out_value = self.model(state)\n",
    "\n",
    "#             policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "#             value_loss = F.mse_loss(out_value, value_targets)\n",
    "#             loss = policy_loss + value_loss\n",
    "\n",
    "#             #minimize the loss by backpropagating it\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "\n",
    "#     def learn(self):\n",
    "#         for iterations in range(self.args['num_iterations']):\n",
    "#             # print(\"iteration number: \", iterations)\n",
    "#             memory = []\n",
    "\n",
    "#             self.model.eval()\n",
    "\n",
    "#             for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']): #trange was used so that we can visualise the progress bars\n",
    "#                 # print(\"selfplay iterations: \", selfPlay_iteration)\n",
    "#                 memory += self.selfPlay()\n",
    "\n",
    "#             self.model.train() \n",
    "#             for epoch in trange(self.args['num_epochs']):\n",
    "#                 # print(\"epoch\", epoch)\n",
    "#                 self.train(memory)\n",
    "\n",
    "#             #store the weights of the model\n",
    "#             torch.save(self.model.state_dict(), f\"model_{iterations}_Attax.pt\")\n",
    "#             torch.save(self.optimizer.state_dict(), f\"optimizer_{iterations}_Attax.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlphaZero:\n",
    "#     def __init__(self, model, optimizer, game, args):\n",
    "#         self.model = model\n",
    "#         self.game = game\n",
    "#         self.optimizer = optimizer\n",
    "#         self.args = args\n",
    "#         self.mcts = MCTS(game, args, 1, model)\n",
    "\n",
    "#     def selfPlay(self):\n",
    "#         memory = []\n",
    "#         player = 1\n",
    "#         state = self.game.get_initial_state()\n",
    "\n",
    "#         while True:\n",
    "#             neutral_state = self.game.change_perspective(state, player)\n",
    "#             action_probs = self.mcts.search(neutral_state)\n",
    "#             print(action_probs)\n",
    "#             memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "#             action = np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
    "\n",
    "#             state = self.game.get_next_state(state, action)\n",
    "\n",
    "#             value,_, is_terminal = self.game.get_value_and_terminated(state, player)\n",
    "\n",
    "#             if is_terminal:\n",
    "#                 returnMemory = []\n",
    "#                 for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "#                     hist_outcome = value if hist_player == player else -value\n",
    "#                     returnMemory.append((\n",
    "#                         self.game.get_encoded_state(neutral_state),\n",
    "#                         hist_action_probs,\n",
    "#                         hist_outcome\n",
    "#                     ))\n",
    "#                 return returnMemory\n",
    "            \n",
    "#             player = -player\n",
    "\n",
    "#     def train(self, memory):\n",
    "#         #shuffle our trainning data\n",
    "#         random.shuffle(memory)\n",
    "\n",
    "#         #loop over batches\n",
    "#         for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "#             sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])]\n",
    "#             state, policy_targets, value_targets = zip(*sample) #transpose our sample arround\n",
    "\n",
    "#             state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "#             state = torch.tensor(state, dtype=torch.float32, device = self.model.device)\n",
    "#             policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device = self.model.device)\n",
    "#             value_targets = torch.tensor(value_targets, dtype=torch.float32, device = self.model.device)\n",
    "\n",
    "#             out_policy, out_value = self.model(state)\n",
    "\n",
    "#             policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "#             value_loss = F.mse_loss(out_value, value_targets)\n",
    "#             loss = policy_loss + value_loss\n",
    "\n",
    "#             #minimize the loss by backpropagating it\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "\n",
    "#     def learn(self):\n",
    "#         for iterations in range(self.args['num_iterations']):\n",
    "#             memory = []\n",
    "\n",
    "#             self.model.eval()\n",
    "\n",
    "#             for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']): #trange was used so that we can visualise the progress bars\n",
    "#                 memory += self.selfPlay()\n",
    "\n",
    "#             self.model.train() \n",
    "#             for epoch in trange(self.args['num_epochs']):\n",
    "#                 self.train(memory)\n",
    "\n",
    "#             #store the weights of the model\n",
    "#             torch.save(self.model.state_dict(), f\"model_{iterations}_{self.game}.pt\")\n",
    "#             torch.save(self.optimizer.state_dict(), f\"optimizer_{iterations}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Código antigo do tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use TensorBoard with your AlphaZero implementation, you'll need to make use of the TensorBoard logging through `SummaryWriter` from the `torch.utils.tensorboard` module. Below is a step-by-step guide to integrate TensorBoard into your existing AlphaZero code.\n",
    "\n",
    "# 1. **Install TensorBoard**:\n",
    "#    Ensure you have TensorBoard installed in your environment. If not, you can install it via pip:\n",
    "\n",
    "#     ```bash\n",
    "#     pip install tensorboard\n",
    "#     ```\n",
    "\n",
    "# 2. **Import SummaryWriter**:\n",
    "#    At the beginning of your script, import the necessary TensorBoard class:\n",
    "\n",
    "#     ```python\n",
    "#     from torch.utils.tensorboard import SummaryWriter\n",
    "#     ```\n",
    "\n",
    "# 3. **Initialize SummaryWriter**:\n",
    "#    Create a `SummaryWriter` instance at the beginning of your training script. This object will be used to write logs into a directory that TensorBoard will later read from.\n",
    "\n",
    "#     ```python\n",
    "#     writer = SummaryWriter('runs/alphazero_experiment_1')\n",
    "#     ```\n",
    "\n",
    "# 4. **Log Data**:\n",
    "#    Throughout your training loop and other functions, use the `writer` to log data, such as loss, accuracy, or custom metrics. Here's how you might incorporate it into your AlphaZero training loop:\n",
    "\n",
    "#     ```python\n",
    "#     def train(self, memory):\n",
    "#         # ... [your existing code] ...\n",
    "        \n",
    "#         # Loop over batches\n",
    "#         for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "#             # ... [your existing code] ...\n",
    "            \n",
    "#             # After optimizer step, log the losses\n",
    "#             writer.add_scalar('Loss/policy', policy_loss.item(), global_step)\n",
    "#             writer.add_scalar('Loss/value', value_loss.item(), global_step)\n",
    "#             writer.add_scalar('Loss/total', loss.item(), global_step)\n",
    "            \n",
    "#             # Increment your global step counter\n",
    "#             global_step += 1\n",
    "#     ```\n",
    "\n",
    "# 5. **Log Custom Metrics and Visualizations**:\n",
    "#    Besides scalar values, you might want to log histograms of parameters, images of the game board, or distributions of move probabilities:\n",
    "\n",
    "#     ```python\n",
    "#     # Log parameters (histograms)\n",
    "#     for name, param in self.model.named_parameters():\n",
    "#         writer.add_histogram(name, param, global_step)\n",
    "\n",
    "#     # Log example game states as images\n",
    "#     # Convert your game state to an image (assuming you have a function for this)\n",
    "#     img = game_state_to_image(state)  \n",
    "#     writer.add_image('Game/Board', img, global_step)\n",
    "#     ```\n",
    "\n",
    "# 6. **Start TensorBoard**:\n",
    "#    Once your script is running and logging data, start TensorBoard in a terminal pointing it to the directory where the logs are being written:\n",
    "\n",
    "#     ```bash\n",
    "#     tensorboard --logdir=runs\n",
    "#     ```\n",
    "\n",
    "# 7. **View Your Logs**:\n",
    "#    Open your browser and go to `localhost:6006` (or the URL provided in the terminal when you start TensorBoard) to view the logs and visualizations.\n",
    "\n",
    "# 8. **Close SummaryWriter**:\n",
    "#    At the end of training, or when you're done logging data, ensure to close the SummaryWriter to flush any remaining outputs to disk:\n",
    "\n",
    "#     ```python\n",
    "#     writer.close()\n",
    "#     ```\n",
    "\n",
    "# By following these steps, you'll be able to integrate TensorBoard into your AlphaZero model for rich logging and visualization capabilities. This will help you monitor your training process, understand your model's behavior, and make informed decisions to improve its performance. Remember to customize the logging according to what's most relevant for your specific scenario and model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
