{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2489ef86-6348-4639-b162-9dd16cf8a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0) # set the seed as 0\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0534730f-0b01-43bf-b1b0-5c09b268926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AttaxGame.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36139d53",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search (MCTS) Implementation Documentation\n",
    "\n",
    "### Overview\n",
    "\n",
    "The MCTS algorithm is a decision-making algorithm that uses Monte Carlo simulations to build a tree of possible moves in a game, searching for the most promising actions to take. This implementation consists of two classes: `Node` representing a node in the MCTS tree, and `MCTS` implementing the MCTS algorithm.\n",
    "\n",
    "### Node Class\n",
    "\n",
    "### Key Decisions\n",
    "\n",
    "#### 1. Node Expansion\n",
    "\n",
    "Nodes are expanded based on the policy predicted by the neural network model. Each child represents a possible action. This decision allows the MCTS algorithm to explore potential moves and build a comprehensive tree of possibilities.\n",
    "\n",
    "#### 2. Selection Strategy\n",
    "\n",
    "Nodes are selected based on the Upper Confidence Bound (UCB) score, striking a balance between exploration and exploitation. This strategy ensures that promising nodes are prioritized for further exploration while still considering less-explored options.\n",
    "\n",
    "#### 3. Simulation\n",
    "\n",
    "Simulations involve random rollouts from a node to estimate the value of a state. This decision introduces an element of randomness, allowing the algorithm to assess the potential outcomes of different actions.\n",
    "\n",
    "#### 4. Backpropagation\n",
    "\n",
    "The value is backpropagated up the tree to update visit counts and value estimates. Backpropagation ensures that the information gathered during simulations influences the overall understanding of the state values throughout the tree.\n",
    "\n",
    "#### 5. Terminal States Handling\n",
    "\n",
    "Terminal states are identified during the simulation phase, and values are flipped to account for the perspective of the opponent. This handling of terminal states ensures accurate value assessments, considering the game outcome from the player's perspective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326be1a-0927-4a1a-af7e-935fa4d3ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, player, parent=None, action_taken=None, prior=0): # =None beacuse of the root node\n",
    "        self.game = game # The game object\n",
    "        self.args = args  # The arguments of the game\n",
    "        self.state = state # The state of the game at this node\n",
    "        self.parent = parent # The parent node of this node\n",
    "        self.action_taken = action_taken # The action that led to this node\n",
    "        self.player = player # The player who made the action\n",
    "        self.prior = prior # The probability of the action taken\n",
    "\n",
    "        self.children = [] # The children of this node\n",
    "\n",
    "        self.visit_count = 0 # The number of times this node has been visited\n",
    "        self.value_sum = 0 # The sum of the values of the children of this node\n",
    "\n",
    "    # Check if the node is fully expanded\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    # Selection phase\n",
    "    def select(self):\n",
    "        # Select the child with the highest ucb score and return it \n",
    "        best_child = None \n",
    "        best_ucb = -np.inf # -inf because we want to maximize the ucb score\n",
    "\n",
    "        # Iterate over the children of this node\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child) # Get the ucb score of the child\n",
    "            if ucb > best_ucb: # If the ucb score is better than the best ucb score\n",
    "                best_child = child  # Set the best child to this child\n",
    "                best_ucb = ucb # Set the best ucb score to this ucb score\n",
    "                \n",
    "        return best_child\n",
    "\n",
    "    # Get the ucb score of a child\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0: # If the child has not been visited\n",
    "            q_value = 0 # Set the q value to 0\n",
    "        else:\n",
    "            # 1- beacuse the next player to make a move is our opponent so we want to put him on a bad situation therefor the value is close to 0\n",
    "            # +1) / 2 is to become a probability, which means the range is [0,1], before it was [-1,1]\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior #the formula of ucb\n",
    "    \n",
    "    # Function to create movement object\n",
    "    def create_mov(self, action, player):\n",
    "        # Extracts xi, yi, xf, yf from action\n",
    "        xi = int(action[0])\n",
    "        yi = int(action[1])\n",
    "        xf = int(action[3])\n",
    "        yf = int(action[4])\n",
    "    \n",
    "        mov = game.movement(xi, yi, xf, yf, player) # Create the movement object\n",
    "        return mov # Return the movement object\n",
    "        \n",
    "    # Expansion phase\n",
    "    def expand(self, policy):\n",
    "        # Iterate over the policy\n",
    "        for action in policy:\n",
    "            prob = policy[action] # Get the probability of the action\n",
    "            if prob > 0: \n",
    "                mov = self.create_mov(action, self.player) # Create the movement object\n",
    "        \n",
    "                child_state = self.state.copy() # Copy the state of this node\n",
    "                child_state = self.game.get_next_state(child_state, mov) # Get the next state of the game\n",
    "                child_state = self.game.change_perspective(child_state, player = -1) # Change the perspective of the state\n",
    "                \n",
    "                child = Node(self.game, self.args, child_state, self.player, self, mov, prob) # Create the child node\n",
    "                self.children.append(child) # Append the child node to the children of this node\n",
    "\n",
    "    # Simulation phase\n",
    "    def simulate(self):\n",
    "        # Get the value of the state and if the game has terminated\n",
    "        value, _, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "        value = -value # Flip the value because we want to maximize the value of the player who made the action\n",
    "\n",
    "        if is_terminal:\n",
    "            return value # If the game has terminated return the value\n",
    "\n",
    "        rollout_state = self.state.copy() # Copy the state of this node\n",
    "        rollout_player = 1 # Set the player to 1\n",
    "        while True:\n",
    "            valid_moves = self.game.get_valid_moves(rollout_state) # Get the valid moves of the state\n",
    "            action = np.random.choice(self.expandable_moves) # Choose a random action from the valid moves\n",
    "\n",
    "            mov = self.create_mov(action, self.player) # Create the movement object\n",
    "            mov.player = rollout_player # Set the player of the movement object to the rollout player\n",
    "            \n",
    "            rollout_state = self.game.get_next_state(rollout_state, mov) # Get the next state of the game\n",
    "            value, _, is_terminal = self.game.get_value_and_terminated(rollout_state, action) # Get the value of the state and if the game has terminated\n",
    "            \n",
    "            if is_terminal: # If the game has terminated\n",
    "                if rollout_player == -1: # If the rollout player is -1\n",
    "                    value = -value # Flip the value\n",
    "                return value # Return the value\n",
    "            \n",
    "            rollout_player = -rollout_player # Flip the rollout player\n",
    "\n",
    "    # Backpropagation phase\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value # Add the value to the value sum\n",
    "        self.visit_count += 1 # Increment the visit count\n",
    "\n",
    "        value = -value # Flip the value because we want to maximize the value of the player who made the action\n",
    "\n",
    "        if self.parent is not None: # If this node is not the root node\n",
    "            self.parent.backpropagate(value) # Backpropagate to the parent node\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e072c8-18de-4cbe-b011-53c486b3c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MCTS class\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, player, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.player = player\n",
    "        self.model = model # The model to use for the MCTS\n",
    "    \n",
    "    @torch.no_grad()\n",
    "\n",
    "    # Search phase\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, self.player) # Create the root node\n",
    "        \n",
    "        for search in range(self.args['num_searches']): # Iterate over the number of searches\n",
    "            node = root # Set the node to the root node\n",
    "            \n",
    "            # Selection phase\n",
    "            while node.is_fully_expanded(): \n",
    "                node = node.select() # Select the child node with the highest ucb score\n",
    "                \n",
    "            # Check if the node is terminal and backpropagate immediately if it is\n",
    "            value, _, is_terminal = self.game.get_value_and_terminated(node.state, node.player)\n",
    "            value = -value  # Flip the value because we want to maximize the value of the player who made the action\n",
    "          \n",
    "            # Check if the node is terminate and backpropagate immediately if not we expand and simulate\n",
    "            if not is_terminal: \n",
    "                # Expansion phase\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0) # Unsqueeze to add a batch dimension\n",
    "                )\n",
    "                # Expand the node with the policy and simulate\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy() # Softmax the policy and convert it to a numpy array\n",
    "                valid_moves = self.game.get_valid_moves(node.state, node.player) # Get the valid moves of the state\n",
    "\n",
    "                policy = {move: value for move, value in zip(valid_moves, policy)} # Create a dictionary of the policy\n",
    "                \n",
    "                \n",
    "                value = value.item() # Get the value from the tensor\n",
    "                node.expand(policy) # Expand the node\n",
    "               \n",
    "            #backpropagation phase\n",
    "            node.backpropagate(value)\n",
    "        \n",
    "        # Get the action probabilities of the root node\n",
    "        action_probs = {} # Create a dictionary of the action probabilities\n",
    "        for child in root.children: # Iterate over the children of the root node\n",
    "            action_probs [child.action_taken] = child.visit_count # Set the action probability of the action taken to the visit count of the child node\n",
    "            \n",
    "        total_visits = sum(action_probs.values()) # Get the total visits of the action probabilities\n",
    "        action_probs = {action: count / total_visits for action, count in action_probs.items()}  # Normalize the action probabilities\n",
    "\n",
    "        # print(action_probs)\n",
    "        return action_probs # Return the action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cf506-796d-47c3-920d-cae992385875",
   "metadata": {},
   "source": [
    "The version below is without the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b25de1-a99f-44eb-8d11-34291d0708a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_without_nn:\n",
    "    def __init__(self, game, args, state, player, parent=None, action_taken=None): # =None beacuse of the root node\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.player = player\n",
    "\n",
    "        self.children = []\n",
    "        self.expandable_moves = game.get_valid_moves(state, player)  #list\n",
    "\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "    #for the expansion\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "\n",
    "    #for the selection\n",
    "    def select(self):\n",
    "        #look of all of your children and for each child we calculate the ucb score and choose the one with the best score\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "\n",
    "    # calculate the ucb score of a node\n",
    "    def get_ucb(self, child):\n",
    "        # 1- beacuse the next player to make a move is our opponent so we want to put him on a bad situation therefor the value is close to 0\n",
    "        # +1) / 2 is to become a probability, which means the range is [0,1], before it was [-1,1]\n",
    "        q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "        # print(self.visit_count)\n",
    "        return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count) / child.visit_count) #the formula of ucb\n",
    "    \n",
    "    def create_mov(self, action, player):\n",
    "        # Extracts xi, yi, xf, yf from action\n",
    "        xi = int(action[0])\n",
    "        yi = int(action[1])\n",
    "        xf = int(action[3])\n",
    "        yf = int(action[4])\n",
    "    \n",
    "        mov = game.movement(xi, yi, xf, yf, player)\n",
    "        return mov\n",
    "        \n",
    "    #expansion\n",
    "    def expand(self):\n",
    "        #== 1 because 1 means the move is legal\n",
    "        #choose randommly an indice of a move to expand\n",
    "        action = np.random.choice(self.expandable_moves) \n",
    "\n",
    "        #make this move not expandable anymore\n",
    "        self.expandable_moves.remove(action)\n",
    "        mov = self.create_mov(action, self.player)\n",
    "\n",
    "        child_state = self.state.copy()\n",
    "        child_state = self.game.get_next_state(child_state, mov) #we never change the player, we flip the state arround \n",
    "        child_state = self.game.change_perspective(child_state, player = -1)\n",
    "        \n",
    "        child = Node_without_nn(self.game, self.args, child_state, player, self, mov)\n",
    "        #append the node\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "\n",
    "    #simulation\n",
    "    def simulate(self):\n",
    "        #verify if it is terminal\n",
    "        value, _, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "        #flip arround\n",
    "        value = -value\n",
    "\n",
    "        if is_terminal:\n",
    "            return value\n",
    "\n",
    "        rollout_state = self.state.copy()\n",
    "        rollout_player = 1\n",
    "        while True:\n",
    "            valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "            action = np.random.choice(self.expandable_moves) \n",
    "\n",
    "            mov = self.create_mov(action, self.player)\n",
    "            mov.player = rollout_player\n",
    "            \n",
    "            rollout_state = self.game.get_next_state(rollout_state, mov)\n",
    "            value, _, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                if rollout_player == -1:\n",
    "                    value = -value\n",
    "                return value\n",
    "            #flip the player\n",
    "            rollout_player = -rollout_player\n",
    "\n",
    "    #backpropagation\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        value = -value\n",
    "        if self.parent is not None:\n",
    "            # print(\"parent:\",self.parent)\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "\n",
    "class MCTS_without_nn:\n",
    "    def __init__(self, game, args, player):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.player = player\n",
    "\n",
    "    def search(self, state):\n",
    "        #define the root node\n",
    "        root = Node_without_nn(self.game, self.args, state, self.player)\n",
    "        \n",
    "        #iterations\n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            #selection phase\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            #check if the node selected is a terminal one \n",
    "            value, _, is_terminal = self.game.get_value_and_terminated(node.state, node.player)\n",
    "            value = -value \n",
    "            # print(\"action taken\",node.action_taken)\n",
    "            # print(is_terminal)\n",
    "            # check if the node is terminate and backpropagate immediately if not we expand and simulate\n",
    "            if not is_terminal: \n",
    "                #expansion phase\n",
    "                node = node.expand()\n",
    "                #simulations phase\n",
    "                value = node.simulate()\n",
    "            # print(value)    \n",
    "            #backpropagation phase\n",
    "            node.backpropagate(value)\n",
    "        \n",
    "        # return the distibution of visit_counts\n",
    "        action_probs = {}\n",
    "        for child in root.children:\n",
    "            action_probs [child.action_taken] = child.visit_count\n",
    "            \n",
    "        total_visits = sum(action_probs.values())\n",
    "        action_probs = {action: count / total_visits for action, count in action_probs.items()}  # Normalize os valores\n",
    "\n",
    "        # print(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a24892-7d35-4a24-9bf5-7de14e25646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = AttaxGame() # Create the game object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
